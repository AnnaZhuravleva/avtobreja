{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic axis method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель метода - отранжировать слова в порядке изменения полярности, от негативной к позитивной или наоборот. Метод опирается на векторы базовых слов каждой полярности (сиды)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(os.path.join(\"wv\", \"model.bin\"), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом мы четыре новых эмбеддинга, который будет предствлять репрезентацию сидов каждой полярности. Для этого мы считываем наши сиды - по каждой тематике отдельно. Затем для каждой тематики усередняем токены для каждой полярности по этой формуле:\n",
    "\n",
    "$$V^+ = \\frac{1}{n}\\sum_{1}^n E(w_{i}^+)$$\n",
    "$$V^- = \\frac{1}{m}\\sum_{1}^m E(w_{i}^-)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_food = []\n",
    "negative_food = []\n",
    "positive_service = []\n",
    "negative_service = []\n",
    "\n",
    "with open(os.path.join(\"seeds\", \"Food_words.txt\"), \"r\") as fd:\n",
    "    food_words = fd.read().split(\"\\n\")\n",
    "\n",
    "for word in food_words:\n",
    "    line = word.split()\n",
    "    if line[-1] == \"1\":\n",
    "        positive_food.append(model[line[1]])\n",
    "    else:\n",
    "        negative_food.append(model[line[1]]) \n",
    "\n",
    "with open(os.path.join(\"seeds\", \"Service_words.txt\"), \"r\") as fd:\n",
    "    food_words = fd.read().split(\"\\n\")\n",
    "\n",
    "for word in food_words:\n",
    "    line = word.split()\n",
    "    if line[-1] == \"1\":\n",
    "        positive_service.append(model[line[1]])\n",
    "    else:\n",
    "        negative_service.append(model[line[1]]) \n",
    "        \n",
    "center_pos_food = np.mean(positive_food, axis=0)\n",
    "center_neg_food = np.mean(negative_food, axis=0)\n",
    "center_pos_service = np.mean(positive_service, axis=0)\n",
    "center_neg_service = np.mean(negative_service, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(center_pos_food.shape)\n",
    "print(center_neg_food.shape)\n",
    "print(center_pos_service.shape)\n",
    "print(center_neg_service.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы создаем \"семантические оси\" для каждой тематикм по следующей формуле:\n",
    "\n",
    "$$V_{axis} = V^+ - V^-$$\n",
    "\n",
    "Итого, мы имеем две оси. Для каждого токена в корпусе высчитываем косинусную близость с данной осью - чем ниже близость, тем более положительно поляризовано слово. Таким образом, мы можем отранжировать все токены в порядке увеличения положительности полярности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_axis_food = center_pos_food - center_neg_food\n",
    "sem_axis_service = center_pos_service - center_neg_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0010059647029266\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(cosine(model[\"вкусно_ADV\"], sem_axis_food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedWord:\n",
    "    def __init__(self, word, cosine, pos):\n",
    "        self.word = word\n",
    "        self.cosine = cosine\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.cosine < other.cosine\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.cosine == other.cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19034/19034 [01:28<00:00, 215.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "weighted_food = []\n",
    "processed_tokens = []\n",
    "weighted_service = []\n",
    "\n",
    "allowed_tags = [\"VERB\", \"NOUN\", \"ADJ\"]\n",
    "\n",
    "for file in tqdm(os.listdir(\"parsed_train\")):\n",
    "    fd = open(os.path.join(\"parsed_train\", file), \"r\")\n",
    "    conllu_text = parse(fd.read())\n",
    "    for sentence in conllu_text:\n",
    "        for word in sentence:\n",
    "            model_lemma = word['lemma'].lower().replace(\"&quot\", \"\") + '_' + word['upostag']        \n",
    "            if model_lemma in processed_tokens:\n",
    "                continue\n",
    "\n",
    "            if word[\"upostag\"] in allowed_tags and model_lemma in model:\n",
    "                weighted_food.append(WeightedWord(model_lemma, \n",
    "                                                 cosine(model[model_lemma], sem_axis_food),\n",
    "                                                 word[\"upostag\"]\n",
    "                                                ))\n",
    "                weighted_service.append(WeightedWord(model_lemma, \n",
    "                                                    cosine(model[model_lemma], sem_axis_service),\n",
    "                                                    word[\"upostag\"]))\n",
    "                processed_tokens.append(model_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_food = sorted(weighted_food)\n",
    "sorted_service = sorted(weighted_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"word_lists\", \"semantic_axis_method\", \"food.csv\"), \"w\") as final_fd:\n",
    "    for word in sorted_food:\n",
    "        final_fd.write(word.word + \"\\t\" + str(word.cosine) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(\"word_lists\", \"semantic_axis_method\", \"service.csv\"), \"w\") as final_fd:\n",
    "    for word in sorted_service:\n",
    "        final_fd.write(word.word + \"\\t\" + str(word.cosine) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили отранжированный список токенов, отдельный по каждой тематике. Посмотрим, какие токены у нас самые маркированные. Мы предполагаем, что списки для еды и для сервиса будут очень похожи, так как мы имеем довольно мало сидов. К тому же, эти сиды неспецифицированные (например, \"плохой\" подходит и для еды, и для сервиса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~FOOD~~~\n",
      "MAX:\n",
      "плохо_NOUN 1.4711369276046753\n",
      "нехороший_ADJ 1.4964621663093567\n",
      "плохой_NOUN 1.5298885107040405\n",
      "плохо_ADJ 1.6258410215377808\n",
      "плохой_ADJ 1.6771029233932495\n",
      "MIN:\n",
      "величественный_ADJ 0.5564294457435608\n",
      "великолепный_ADJ 0.5609675943851471\n",
      "обширный_ADJ 0.5676152110099792\n",
      "разнообразный_ADJ 0.5677912831306458\n",
      "живописный_ADJ 0.5748997330665588\n",
      "\n",
      "\n",
      "~~~SERVICE~~~\n",
      "MAX:\n",
      "люмпен_NOUN 1.2843709290027618\n",
      "орать_NOUN 1.2850382924079895\n",
      "обозвать_VERB 1.285082459449768\n",
      "дезертир_NOUN 1.296842485666275\n",
      "деморализованный_ADJ 1.2989414930343628\n",
      "MIN:\n",
      "приятный_ADJ 0.3453672528266907\n",
      "приветливый_ADJ 0.36830490827560425\n",
      "радушный_ADJ 0.4070584177970886\n",
      "дружелюбный_ADJ 0.4409315586090088\n",
      "милый_ADJ 0.442871630191803\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~FOOD~~~\")\n",
    "print(\"MAX:\")\n",
    "for word in sorted_food[-5:]:\n",
    "    print(word.word, word.cosine)\n",
    "print(\"MIN:\")\n",
    "for word in sorted_food[:5]:\n",
    "    print(word.word, word.cosine)\n",
    "    \n",
    "\n",
    "print(\"\\n\\n~~~SERVICE~~~\")\n",
    "print(\"MAX:\")\n",
    "for word in sorted_service[-5:]:\n",
    "    print(word.word, word.cosine)\n",
    "print(\"MIN:\")\n",
    "for word in sorted_service[:5]:\n",
    "    print(word.word, word.cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем z-score нормализацию значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food mean 0.9976846222538592\n",
      "food std 0.10944905145854601\n",
      "service mean 1.005963000205442\n",
      "service std 0.09811330958646661\n"
     ]
    }
   ],
   "source": [
    "food_mean = np.mean([word.cosine for word in sorted_food])\n",
    "service_mean = np.mean([word.cosine for word in sorted_service])\n",
    "\n",
    "food_std = np.std([word.cosine for word in sorted_food])\n",
    "service_std = np.std([word.cosine for word in sorted_service])\n",
    "\n",
    "print(\"food mean\", food_mean)\n",
    "print(\"food std\", food_std)\n",
    "print(\"service mean\", service_mean)\n",
    "print(\"service std\", service_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def z_score_normalization(word_list, list_mean, list_std, if_counter=False):\n",
    "    normalized_list = {}\n",
    "    for word in word_list:\n",
    "        if if_counter:\n",
    "            normalized_list[word] = (word_list[word] - list_mean) / list_std\n",
    "        else:\n",
    "            normalized_list[word.word] = (word.cosine - list_mean) / list_std\n",
    "    return Counter(normalized_list)\n",
    "        \n",
    "norm_food = z_score_normalization(sorted_food, food_mean, food_std)\n",
    "norm_service = z_score_normalization(sorted_service, service_mean, service_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~FOOD~~~\n",
      "MIN:\n",
      "('живописный_ADJ', -3.8628465350147945)\n",
      "('разнообразный_ADJ', -3.927794105059341)\n",
      "('обширный_ADJ', -3.9294028181392635)\n",
      "('великолепный_ADJ', -3.990139905726998)\n",
      "('величественный_ADJ', -4.031603477874127)\n",
      "MAX:\n",
      "('плохой_ADJ', 6.207621647563761)\n",
      "('плохо_ADJ', 5.739258503504132)\n",
      "('плохой_NOUN', 4.862571957983156)\n",
      "('нехороший_ADJ', 4.5571664387097055)\n",
      "('плохо_NOUN', 4.325778058754002)\n",
      "\n",
      "\n",
      "~~~SERVICE~~~\n",
      "MIN:\n",
      "('милый_ADJ', -5.739194533208467)\n",
      "('дружелюбный_ADJ', -5.758968319160356)\n",
      "('радушный_ADJ', -6.1042134337599)\n",
      "('приветливый_ADJ', -6.499200716166587)\n",
      "('приятный_ADJ', -6.732988115099437)\n",
      "MAX:\n",
      "('деморализованный_ADJ', 2.9861238405246207)\n",
      "('дезертир_NOUN', 2.9647301338304444)\n",
      "('обозвать_VERB', 2.844868452820256)\n",
      "('орать_NOUN', 2.844418289208767)\n",
      "('люмпен_NOUN', 2.8376163231142533)\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~FOOD~~~\")\n",
    "print(\"MIN:\")\n",
    "for word in norm_food.most_common()[-5:]:\n",
    "    print(word)\n",
    "print(\"MAX:\")\n",
    "for word in norm_food.most_common(5):\n",
    "    print(word)\n",
    "    \n",
    "\n",
    "print(\"\\n\\n~~~SERVICE~~~\")\n",
    "print(\"MIN:\")\n",
    "for word in norm_service.most_common()[-5:]:\n",
    "    print(word)\n",
    "print(\"MAX:\")\n",
    "for word in norm_service.most_common(5):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы хотим получить конечные списки токенов, учтя пересечение всех использованных нами методов. Для этого мы соответственно сложим все метрики для каждого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(\"word_lists\", \"CNN\", 'sent_food.pickle'), 'rb') as f:\n",
    "    food_list = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(\"word_lists\", \"CNN\", 'sent_service.pickle'), 'rb') as f:\n",
    "    service_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metric(counter_list):\n",
    "\n",
    "    average_metric = {}\n",
    "\n",
    "    for counter in counter_list:\n",
    "        for token in counter:\n",
    "            if token not in average_metric:\n",
    "                average_metric[token] = [counter[token]]\n",
    "            else:\n",
    "                average_metric[token].append(counter[token])\n",
    "                \n",
    "    for key in average_metric:\n",
    "        average_metric[key] = np.mean(average_metric[key])\n",
    "    return Counter(average_metric)\n",
    "\n",
    "average_food = mean_metric(food_list)\n",
    "average_service = mean_metric(service_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~FOOD~~~\n",
      "MAX:\n",
      "('очень_ADV', 11437.910938599616)\n",
      "('вкусный_ADJ', 9284.592686381122)\n",
      "('интерьер_NOUN', 4962.086438708173)\n",
      "('приятный_ADJ', 4419.284605484837)\n",
      "('год_NOUN', 3819.742681226211)\n",
      "MIN:\n",
      "('оценка_NOUN', -74.68075154055259)\n",
      "('просить_VERB', -81.60893007901905)\n",
      "('посетитель_NOUN', -125.6364262850575)\n",
      "('помещение_NOUN', -147.90723155334854)\n",
      "('пойти_VERB', -183.5469616511209)\n",
      "\n",
      "\n",
      "~~~SERVICE~~~\n",
      "MAX:\n",
      "('очень_ADV', 9962.569364785297)\n",
      "('приятный_ADJ', 6230.708526983835)\n",
      "('вкусный_ADJ', 5238.680879248544)\n",
      "('уютный_ADJ', 4438.081501664099)\n",
      "('атмосфера_NOUN', 4315.923531234968)\n",
      "MIN:\n",
      "('один_NUM', -198.5470799011896)\n",
      "('столик_NOUN', -231.35559087586626)\n",
      "('гость_NOUN', -292.2447034124025)\n",
      "('внимание_NOUN', -295.53615339319185)\n",
      "('стол_NOUN', -524.0081696923007)\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~FOOD~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in average_food.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in average_food.most_common()[-5:]:\n",
    "    print(el)\n",
    "    \n",
    "print(\"\\n\\n~~~SERVICE~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in average_service.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in average_service.most_common()[-5:]:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food mean 3.4863766971231622\n",
      "food std 68.84006361884803\n",
      "service mean 3.7298273491683043\n",
      "service std 66.60644373501773\n"
     ]
    }
   ],
   "source": [
    "average_food_mean = np.mean([average_food[word] for word in average_food])\n",
    "average_service_mean = np.mean([average_service[word] for word in average_service])\n",
    "\n",
    "average_food_std = np.std([average_food[word] for word in average_food])\n",
    "average_service_std = np.std([average_service[word] for word in average_service])\n",
    "\n",
    "print(\"food mean\", average_food_mean)\n",
    "print(\"food std\", average_food_std)\n",
    "print(\"service mean\", average_service_mean)\n",
    "print(\"service std\", average_service_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_norm_food = z_score_normalization(average_food, average_food_mean, average_food_std, if_counter=True)\n",
    "average_norm_service = z_score_normalization(average_service, average_service_mean, average_service_std, if_counter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~FOOD~~~\n",
      "MAX:\n",
      "('очень_ADV', 166.10130730285684)\n",
      "('вкусный_ADJ', 134.82129187258454)\n",
      "('интерьер_NOUN', 72.03073038203021)\n",
      "('приятный_ADJ', 64.14576042865092)\n",
      "('год_NOUN', 55.43655981579044)\n",
      "MIN:\n",
      "('оценка_NOUN', -1.1354889017893648)\n",
      "('просить_VERB', -1.2361305655859909)\n",
      "('посетитель_NOUN', -1.8756926736312243)\n",
      "('помещение_NOUN', -2.1992078492068816)\n",
      "('пойти_VERB', -2.716925704540391)\n",
      "\n",
      "\n",
      "~~~SERVICE~~~\n",
      "MAX:\n",
      "('очень_ADV', 149.51765893785978)\n",
      "('приятный_ADJ', 93.48913333982564)\n",
      "('вкусный_ADJ', 78.59526433697208)\n",
      "('уютный_ADJ', 66.57541561528545)\n",
      "('атмосфера_NOUN', 64.74138930222908)\n",
      "MIN:\n",
      "('один_NUM', -3.036896971336313)\n",
      "('столик_NOUN', -3.5294695984713043)\n",
      "('гость_NOUN', -4.443632089697725)\n",
      "('внимание_NOUN', -4.493048479407463)\n",
      "('стол_NOUN', -7.923227355313906)\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~FOOD~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in average_norm_food.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in average_norm_food.most_common()[-5:]:\n",
    "    print(el)\n",
    "    \n",
    "print(\"\\n\\n~~~SERVICE~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in average_norm_service.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in average_norm_service.most_common()[-5:]:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_score(one_list, another_list):\n",
    "    total_list = {}\n",
    "    for word in one_list:\n",
    "        if word not in another_list:\n",
    "            continue\n",
    "        total_list[word] = one_list[word] * 20 + another_list[word]\n",
    "    return Counter(total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_food = count_total_score(norm_food, average_norm_food)\n",
    "total_service = count_total_score(norm_service, average_norm_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~FOOD~~~\n",
      "MAX:\n",
      "('вкусный_ADJ', 137.47484866640062)\n",
      "('плохой_ADJ', 124.8155285447863)\n",
      "('плохо_ADJ', 114.94069716003143)\n",
      "('плохой_NOUN', 97.20079457112197)\n",
      "('нехороший_ADJ', 91.09427657657248)\n",
      "MIN:\n",
      "('необыкновенный_ADJ', -73.04109101251572)\n",
      "('великолепный_ADJ', -76.08433495459823)\n",
      "('живописный_ADJ', -76.77724693908834)\n",
      "('обширный_ADJ', -77.77615607578247)\n",
      "('величественный_ADJ', -80.63956970724162)\n",
      "\n",
      "\n",
      "~~~SERVICE~~~\n",
      "MAX:\n",
      "('деморализованный_ADJ', 59.66647881480288)\n",
      "('дезертир_NOUN', 59.235652612393906)\n",
      "('обозвать_VERB', 56.83842684217229)\n",
      "('орать_NOUN', 56.83049384777462)\n",
      "('люмпен_NOUN', 56.69632846659554)\n",
      "MIN:\n",
      "('ласковый_ADJ', -110.2697821090971)\n",
      "('любезный_ADJ', -111.02047615590729)\n",
      "('дружелюбный_ADJ', -113.13881994524824)\n",
      "('радушный_ADJ', -119.29214391109717)\n",
      "('приветливый_ADJ', -120.35591199195714)\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~FOOD~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in total_food.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in total_food.most_common()[-5:]:\n",
    "    print(el)\n",
    "    \n",
    "print(\"\\n\\n~~~SERVICE~~~\")\n",
    "print(\"MAX:\")\n",
    "for el in total_service.most_common(5):\n",
    "    print(el)\n",
    "print(\"MIN:\")   \n",
    "for el in total_service.most_common()[-5:]:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31265 29973\n"
     ]
    }
   ],
   "source": [
    "print(len(total_food), len(total_service))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяем по 5% токенов с каждой стороны списка и соответственно маркируем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"word_lists\", \"total_CNN+SAM\", \"food.csv\"), \"w\") as food_fd:\n",
    "    for el in total_food.most_common(int(len(total_service) / 100 * 5)):\n",
    "        food_fd.write(el[0] + \"\\t\" + \"0\\n\")\n",
    "    for el in total_food.most_common()[int(len(total_service) / 100 * 5):]:\n",
    "        food_fd.write(el[0] + \"\\t\" + \"1\\n\")\n",
    "    \n",
    "with open(os.path.join(\"word_lists\", \"total_CNN+SAM\", \"service.csv\"), \"w\") as service_fd:\n",
    "    for el in total_service.most_common(int(len(total_service) / 100 * 5)):\n",
    "        service_fd.write(el[0] + \"\\t\" + \"0\\n\")\n",
    "    for el in total_service.most_common()[int(len(total_service) / 100 * 5):]:\n",
    "        service_fd.write(el[0] + \"\\t\" + \"1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полные списки тоже на всякий сохраним:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"word_lists\", \"total_CNN+SAM\", \"full_food_wo_mark.csv\"), \"w\") as food_fd:\n",
    "    for el in total_food.most_common():\n",
    "        food_fd.write(el[0] + \"\\t\" + str(el[1]) + \"\\n\")\n",
    "    \n",
    "with open(os.path.join(\"word_lists\", \"total_CNN+SAM\", \"full_service_wo_mark.csv\"), \"w\") as service_fd:\n",
    "    for el in total_service.most_common():\n",
    "        service_fd.write(el[0] + \"\\t\" + str(el[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

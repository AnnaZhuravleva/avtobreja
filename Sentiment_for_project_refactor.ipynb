{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment for project refactor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaZhuravleva/avtobreja/blob/master/Sentiment_for_project_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ec9ls9EIzwn_",
        "outputId": "b4756779-afc6-49ec-bd6b-f5c28d8d7cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "!pip install innvestigate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: innvestigate in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from innvestigate) (1.17.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from innvestigate) (4.3.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from innvestigate) (3.6.4)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from innvestigate) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from innvestigate) (2.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from innvestigate) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from innvestigate) (1.3.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->innvestigate) (0.46)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (42.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (8.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (1.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cBt9Gi0PzwoK",
        "outputId": "ea71a7e4-7b35-45a3-80ef-4925a32c7014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.17.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.1.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtO9KcJducxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25c7ec10-72e8-4373-d9a0-3eab7e636c47"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56mr43dU082W",
        "outputId": "1bebf561-8a4c-4f21-d848-8d551e77ab6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "!wget http://vectors.nlpl.eu/repository/11/180.zip\n",
        "!unzip 180.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-21 13:16:12--  http://vectors.nlpl.eu/repository/11/180.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 484452285 (462M) [application/zip]\n",
            "Saving to: ‘180.zip.1’\n",
            "\n",
            "180.zip.1             3%[                    ]  14.97M   272KB/s    eta 40m 36s^C\n",
            "Archive:  180.zip\n",
            "replace README? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace meta.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace model.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x3s8x5jkzwoO",
        "outputId": "96d95d5b-ccba-4fb7-b41c-16c15fb74f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "import innvestigate\n",
        "import keras.backend\n",
        "import keras.models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from collections import Counter\n",
        "from gensim.models import KeyedVectors\n",
        "from itertools import chain\n",
        "from innvestigate.utils.tests.networks import base as network_base\n",
        "from matplotlib import cm, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm44_u2xPWRz",
        "colab_type": "text"
      },
      "source": [
        "## Оценка тональности с помощью CNN\n",
        "\n",
        "В качестве входных представлений будем использовать word2vec для лемм с POS-тегами UD.\n",
        "Архитектура классификатора примерно воспроизводит описанную в [статье Arras et al. 2017](http://www.aclweb.org/anthology/W16-1601); а для визуализации воспользуемся библиотекой [iNNvestigate](https://github.com/albermax/innvestigate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVudE4CIPWSD",
        "colab_type": "text"
      },
      "source": [
        "Картинка про классификатор и оценку значимости входных слов ([источник](https://doi.org/10.1371/journal.pone.0181142.g001)):\n",
        "<img src=\"https://camo.githubusercontent.com/ba37f37fdbb90ccd76f1c4bf399e0cb8ddbc66f0/68747470733a2f2f692e696d6775722e636f6d2f595144665335502e706e67\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTpoNjYn0mkN"
      },
      "source": [
        "### CNN functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1agAEQRgzwoi",
        "colab": {}
      },
      "source": [
        "def max_length(texts):\n",
        "    return max(len(t) for t in texts)\n",
        "    \n",
        "def to_one_hot(y):\n",
        "    return keras.utils.to_categorical(y, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJVzKjWHzwo-",
        "colab": {}
      },
      "source": [
        "def load_dataset(w2v_model, lines, MAX_LEN, num_examples=None):\n",
        "    \"\"\"\n",
        "    embedds a list of texts with a w2v model\n",
        "\n",
        "    :param w2v_model: gensim KeyedVectors model\n",
        "    :param lines: list of lists of str, texts of words (pre-processed to be model-compatible)\n",
        "    :param MAX_LEN: int, maximal text length for padding\n",
        "    :param num_examples: int, number of texts to add to dataset, optional, default None \n",
        "\n",
        "    :return x_tensor: np.ndarray, embedded texts\n",
        "    :return vocab: counter, how many times words occured in texts\n",
        "    \"\"\"\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "    prep = lines[:num_examples]\n",
        "    vocab = Counter()\n",
        "    x_tensor = np.zeros((len(prep), MAX_LEN, embedding_dim))\n",
        "    for i, text in enumerate(prep):\n",
        "        for j, w in enumerate(text):\n",
        "            try:\n",
        "                x_tensor[i, j, :] = w2v_model[w]\n",
        "            except KeyError:\n",
        "                pass\n",
        "        vocab[w] += 1\n",
        "    return x_tensor, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftQSi5hDG_wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset_for_training_cnn(w2v_model, texts, scores):\n",
        "  \"\"\"\n",
        "  vectorizes dataset from a list of texts and scores (do not give more than 2000, as more eats all RAM)\n",
        "\n",
        "  :param w2v_model: gensim KeyedVectors model\n",
        "  :param texts: list of lists of str, texts of words\n",
        "  :param scores: list of int, target text scores\n",
        "\n",
        "  :return MAX_LEN: int, maximal text length for padding\n",
        "  :return scores_train: list of int, traing scores\n",
        "  :return scores_val: list of int, validation scores\n",
        "  :return texts_train: list of lists of str, traing texts\n",
        "  :return texts_val: list of lists of str, validation texts\n",
        "  :return input_tensor_train: np.ndarray, embedded traing texts\n",
        "  :return inp_vocab_train: counter, traing texts vocabulary\n",
        "  :return input_tensor_val: np.ndarray, embedded validation texts\n",
        "  :return inp_vocab_val: counter, validation texts vocabulary\n",
        "  \"\"\"\n",
        "  if len(texts) != len(scores):\n",
        "    raise ValueError(f\"expected texts and scores of the same length but got lengths {len(texts)} and {len(scores)}\")\n",
        "  scores = np.array(scores)\n",
        "  binary_scores = scores > 5.\n",
        "  binary_scores = binary_scores.astype(int)\n",
        "  scores_train, scores_val, texts_train, texts_val = train_test_split(binary_scores, texts, test_size=0.3, shuffle=False)\n",
        "  MAX_LEN = max(max_length(texts_train), max_length(texts_val))\n",
        "  input_tensor_train, inp_vocab_train = load_dataset(w2v_model, texts_train, MAX_LEN)\n",
        "  input_tensor_val, inp_vocab_val = load_dataset(w2v_model, texts_val, MAX_LEN)\n",
        "  return MAX_LEN, scores_train, scores_val, texts_train, texts_val, input_tensor_train, inp_vocab_train, input_tensor_val, inp_vocab_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQiqdpcgzwpv",
        "colab": {}
      },
      "source": [
        "def build_network(max_len, voc_size, embedding_dim, output_n, activation=None, dense_unit=256, dropout_rate=0.25):\n",
        "    \"\"\"\n",
        "    builds a keras cnn structure\n",
        "\n",
        "    :param max_len: int, sequence length\n",
        "    :param voc_size: int, number of words in vocabulary\n",
        "    :param embedding_dim: int, dimensionality of w2v model\n",
        "    :param output_n: int, number of classes\n",
        "    :param activation: pre-trained weights, optional, default None\n",
        "    :param dense_unit: int, optional, dimensionality of the dense unit \n",
        "    :param dropout_rate: float, optional, default=0.25\n",
        "\n",
        "    :return: keras nn\n",
        "    \"\"\"\n",
        "    if activation:\n",
        "        activation = \"relu\"\n",
        "\n",
        "    net = {}\n",
        "    net[\"in\"] = keras.Input(shape=[1, max_len, embedding_dim])\n",
        "    net[\"conv\"] = keras.layers.Conv2D(filters=100, kernel_size=(1,2), strides=(1, 1), padding='valid')(net[\"in\"])\n",
        "    net[\"pool\"] = keras.layers.MaxPooling2D(pool_size=(1, max_len - 1), strides=(1,1))(net[\"conv\"])\n",
        "    net[\"out\"] = network_base.dense_layer(keras.layers.Flatten()(net[\"pool\"]), units=output_n, activation=activation)\n",
        "    net[\"sm_out\"] = network_base.softmax(net[\"out\"])\n",
        "\n",
        "\n",
        "    net.update({\n",
        "        \"input_shape\": [1, max_len, embedding_dim],\n",
        "        \"output_n\": output_n,\n",
        "    })\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIwEjLbJzwp9",
        "colab": {}
      },
      "source": [
        "def train_model(model, input_tensor_train, input_tensor_val, scores_train, scores_val, epochs=20):\n",
        "    \"\"\"\n",
        "    training keras nn\n",
        "\n",
        "    :param model: keras nn\n",
        "    :param input_tensor_train: np.ndarray, training set\n",
        "    :param input_tensor_val: np.ndarray, validation set\n",
        "    :param scores_train: list of int, traing scores\n",
        "    :param scores_val: list of int, validation scores\n",
        "    :param epochs: int, optional, default 20\n",
        "    \"\"\"\n",
        "    x_train = np.expand_dims(input_tensor_train, axis=1)\n",
        "    y_train = to_one_hot(scores_train)\n",
        "    \n",
        "    x_val = np.expand_dims(input_tensor_val, axis=1)\n",
        "    y_val = to_one_hot(scores_val)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=256,\n",
        "                        epochs=epochs,\n",
        "                        verbose=1,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-PfMZ0IZAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(w2v_model, texts_train, texts_val, scores_train, scores_val, inp_vocab_train, inp_vocab_val, input_tensor_train, input_tensor_val, MAX_LEN, epochs=20):\n",
        "  \"\"\"\n",
        "  trains two cnn models form split dataset\n",
        "\n",
        "  :param w2v_model: gensim KeyedVectors model\n",
        "  :param texts_train: list of lists of str, traing texts\n",
        "  :param texts_val: list of lists of str, validation texts\n",
        "  :param scores_train: list of int, traing scores\n",
        "  :param scores_val: list of int, validation scores\n",
        "  :param inp_vocab_train: counter, traing texts vocabulary\n",
        "  :param inp_vocab_val: counter, validation texts vocabulary\n",
        "  :param input_tensor_train: np.ndarray, training set\n",
        "  :param input_tensor_val: np.ndarray, validation set\n",
        "  :param MAX_LEN: int, sequence length\n",
        "  :param epochs: int, optional, default 20\n",
        "\n",
        "  :return model_without_softmax: keras nn, without classifier layer\n",
        "  :return model_with_softmax: keras nn, with classifier layer\n",
        "  :return embedding_dim: int, dimension of w2v\n",
        "  \"\"\"\n",
        "  embedding_dim = w2v_model.vector_size\n",
        "  inp_vocab = inp_vocab_train + inp_vocab_val\n",
        "  vocab_inp_size = len(inp_vocab) + 1\n",
        "  net = build_network(MAX_LEN, vocab_inp_size, embedding_dim, 2)\n",
        "  model_without_softmax = keras.models.Model(inputs=net['in'], outputs=net['out'])\n",
        "  model_with_softmax = keras.models.Model(inputs=net['in'], outputs=net['sm_out'])\n",
        "  train_model(model_with_softmax, input_tensor_train, input_tensor_val, scores_train, scores_val, epochs)\n",
        "  model_without_softmax.set_weights(model_with_softmax.get_weights())\n",
        "  return model_without_softmax, model_with_softmax, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WLSLuolEzwqf",
        "colab": {}
      },
      "source": [
        "def analyze_scores(X, Y, ridx, embedding_dim, model_without_softmax, model_with_softmax, analyzers, max_len):\n",
        "    \"\"\"\n",
        "    predicts individual word scores and text score\n",
        "\n",
        "    :param X: np.ndarray, embedded texts tensor\n",
        "    :param Y: list of int, target classes\n",
        "    :param ridx: int, text id \n",
        "    :param model_without_softmax: keras nn, without classifier layer\n",
        "    :param model_with_softmax: keras nn, with classifier layer\n",
        "    :param embedding_dim: int, dimension of w2v\n",
        "    :param analyzers: list of innvestigate analyzers\n",
        "    :param max_len: int, sequence length\n",
        "\n",
        "    :return analysis: list of list of float, word weights according to each analyzer\n",
        "    :return y_hat: int, calss prediction\n",
        "    \"\"\"\n",
        "    analysis = np.zeros([len(analyzers), 1, max_len])\n",
        "    x, y = X[ridx], Y[ridx]\n",
        "    t_start = time.time()\n",
        "    x = x.reshape((1, 1, max_len, embedding_dim))\n",
        "    presm = model_without_softmax.predict_on_batch(x)[0] #forward pass without softmax\n",
        "    prob = model_with_softmax.predict_on_batch(x)[0] #forward pass with softmax\n",
        "    y_hat = prob.argmax()\n",
        "  \n",
        "    for aidx, analyzer in enumerate(analyzers):\n",
        "        a = np.squeeze(analyzer.analyze(x))\n",
        "        a = np.sum(a, axis=1)\n",
        "        analysis[aidx] = a\n",
        "    t_elapsed = time.time() - t_start\n",
        "    # print('Review %d (%.4fs)'% (ridx, t_elapsed))\n",
        "    return analysis, y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_aban3LZsCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment_vocabulary(w2v_model, texts, scores, sentiment_scores=None, only_correct=True, epochs=10):\n",
        "  \"\"\"\n",
        "  gets sentiment vocabulary from texts\n",
        "\n",
        "  :param w2v_model: gensim KeyedVectors model\n",
        "  :param texts: list of lists of str, texts of words\n",
        "  :param scores: list of int, target text scores \n",
        "  :param sentiment_scores: counter of words to update, optional, default None\n",
        "  :param only_correct: bool, if only use the weights from correct predictions, optional, default True\n",
        "  :param epochs: int, cnn training param, optional, default 10\n",
        "\n",
        "  :return: counter, words and their sentiment weights\n",
        "  \"\"\"\n",
        "  print(\"__making the dataset__\")\n",
        "  MAX_LEN, scores_train, scores_val, texts_train, texts_val, input_tensor_train, inp_vocab_train, input_tensor_val, inp_vocab_val = make_dataset_for_training_cnn(w2v_model, texts, scores)\n",
        "  print(\"__training the network__\")\n",
        "  model_without_softmax, model_with_softmax, embedding_dim = train_network(w2v_model, texts_train, texts_val, scores_train, scores_val, inp_vocab_train, inp_vocab_val, input_tensor_train, input_tensor_val, MAX_LEN, epochs=epochs)\n",
        "\n",
        "  print(\"__making the analyzers__\")\n",
        "  methods = ['gradient', 'lrp.z', 'lrp.alpha_2_beta_1', 'pattern.attribution']\n",
        "  kwargs = [{}, {}, {}, {'pattern_type': 'relu'}]\n",
        "  analyzers = []\n",
        "  for method, kws in zip(methods, kwargs):\n",
        "      analyzer = innvestigate.create_analyzer(method, model_without_softmax, **kws)\n",
        "      analyzer.fit(np.expand_dims(input_tensor_train, axis=1), batch_size=256, verbose=1)\n",
        "      analyzers.append(analyzer)\n",
        "\n",
        "  if not sentiment_scores:\n",
        "    sentiment_scores = [Counter() for method in methods]\n",
        "  \n",
        "  for idx in tqdm(range(len(input_tensor_train)), desc=\"__building the sentiment vocabulary on training set__\"):\n",
        "    words = texts_train[idx]\n",
        "    y_true = scores_train[idx]\n",
        "    a, y_pred = analyze_scores(input_tensor_train, scores_train, idx, embedding_dim, model_without_softmax, model_with_softmax, analyzers, MAX_LEN)\n",
        "    if only_correct and y_true == y_pred or not only_correct:\n",
        "      for j, method in enumerate(methods):\n",
        "        sentiment_scores[j].update(dict(zip(words, a[j].reshape(-1))))\n",
        "\n",
        "  for idx in tqdm(range(len(input_tensor_val)), desc=\"__building the sentiment vocabulary on validation set__\"):\n",
        "    words = texts_val[idx]\n",
        "    y_true = scores_val[idx]\n",
        "    a, y_pred = analyze_scores(input_tensor_val, scores_val, idx, embedding_dim, model_without_softmax, model_with_softmax, analyzers, MAX_LEN)\n",
        "    if only_correct and y_true == y_pred or not only_correct:\n",
        "      for j, method in enumerate(methods):\n",
        "        sentiment_scores[j].update(dict(zip(words, a[j].reshape(-1))))\n",
        "  return sentiment_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoH8gYF60yDK",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFeFPWhajwCH",
        "colab_type": "text"
      },
      "source": [
        "### Данные\n",
        "Каждый текст - строчка из токенов лемма_тег. Оценки отдельно выведены для сервиса, отдельно для еды. Мы будем строить бинарную классификацию, поэтому будем считать оценки выше 5 положительными, а 5 и ниже — отрицательными."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOZWESjuKeCH",
        "colab_type": "code",
        "outputId": "3ad215da-76d0-446c-dfd9-042b311cc0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUqSakq6sai7",
        "colab_type": "code",
        "outputId": "a10de169-daf8-4c61-b054-1a273a8fd021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "scored = pd.read_csv(\"/content/drive/My Drive/studies/HSE/prog/nlp/SentiRuEval_rest_train_lemma_POS.csv\", index_col=0)\n",
        "scored.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>food</th>\n",
              "      <th>service</th>\n",
              "      <th>text</th>\n",
              "      <th>lemma_POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19029</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>С тех пор, как побывала последний раз, мнение ...</td>\n",
              "      <td>с_ADP тот_DET пора_NOUN ,_PUNCT как_SCONJ быва...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19030</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Обнаруженный в салате таракан очень поразил!!!...</td>\n",
              "      <td>обнаружить_VERB в_ADP салат_NOUN таракан_NOUN ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19031</th>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>Забавное заведение. Симпатичный светлый интерь...</td>\n",
              "      <td>забавный_ADJ заведение_NOUN ._PUNCT симпатичны...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19032</th>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>Поздравляем администрацию ресторана Навруз с Н...</td>\n",
              "      <td>поздравлять_VERB администрация_NOUN ресторан_N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19033</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>Очень нравиться интерьер. Отличный гриль.Карта...</td>\n",
              "      <td>очень_ADV нравиться_VERB интерьер_NOUN ._PUNCT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       food  ...                                          lemma_POS\n",
              "19029     2  ...  с_ADP тот_DET пора_NOUN ,_PUNCT как_SCONJ быва...\n",
              "19030     1  ...  обнаружить_VERB в_ADP салат_NOUN таракан_NOUN ...\n",
              "19031     6  ...  забавный_ADJ заведение_NOUN ._PUNCT симпатичны...\n",
              "19032     8  ...  поздравлять_VERB администрация_NOUN ресторан_N...\n",
              "19033     8  ...  очень_ADV нравиться_VERB интерьер_NOUN ._PUNCT...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUd_xMbw0xEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = list(scored[\"lemma_POS\"].apply(lambda s: s.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "636bb65e-e712-4f1a-b0db-190ef49f379c",
        "id": "IryN96Js0mju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "w2v_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oRdaWmdY0mkK"
      },
      "source": [
        "## Food\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_yLrkSHx0mj-",
        "colab": {}
      },
      "source": [
        "scores = scored[\"food\"].apply(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA9aXOw6s6yK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "c713d0cc-5477-481f-d0ea-2e46db448064"
      },
      "source": [
        "sent = get_sentiment_vocabulary(w2v_model, texts[:2000], scores[:2000], epochs=5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__making the dataset__\n",
            "__training the network__\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/1\n",
            "1400/1400 [==============================] - 10s 7ms/step - loss: 1.0096 - acc: 0.6329 - val_loss: 0.7459 - val_acc: 0.7550\n",
            "__making the analyzers__\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/innvestigate/utils/keras/backend.py:126: calling extract_image_patches (from tensorflow.python.ops.array_ops) with ksizes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "ksizes is deprecated, use sizes instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 13s 2s/step - loss: 2.0000 - broadcast_1_loss: 1.0000 - broadcast_2_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:10<00:00, 19.86it/s]\n",
            "  0%|          | 2/600 [00:00<00:30, 19.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:30<00:00, 19.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGghEFbtDCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e2e2673-913e-4f74-d7ee-0bd7febd8703"
      },
      "source": [
        "sent[0]['отличный_ADJ']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.014342761598527431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOHAIzStEzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e377aace-c054-4fc2-ff83-af245e3d7a15"
      },
      "source": [
        "sent[0]['ужасный_ADJ']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.8887263648211956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WGR7bV8tHOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "230e9fb2-e026-4939-827c-8ca125b1eb54"
      },
      "source": [
        "sent[0]['хорошо_ADV']"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.5300868730992079"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut7HrkLOtLNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "783a3bf7-7038-42de-bf4d-0fd716253245"
      },
      "source": [
        "sent[0]['плохо_ADV']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.8013026644475758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy2CGcLFhn0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba046de0-80a1-4b69-bae3-0ffae738e63a"
      },
      "source": [
        "for i in range(2000,len(texts),2000):\n",
        "  print(f\"____________________{i/2000}_________________________\")\n",
        "  if i + 2000 > len(texts):\n",
        "    sent = get_sentiment_vocabulary(w2v_model, texts[i:], scores[i:], sent, epochs=5)\n",
        "  else:\n",
        "    sent = get_sentiment_vocabulary(w2v_model, texts[i:i+2000], scores[i:i+2000], sent, epochs=5)\n",
        "  with open('/content/drive/My Drive/studies/HSE/prog/nlp/sent_food.pickle', 'wb') as f:\n",
        "    pickle.dump(sent, f)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____________________2.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 12s 8ms/step - loss: 1.4295 - acc: 0.6557 - val_loss: 0.9141 - val_acc: 0.5533\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.6709 - acc: 0.7214 - val_loss: 0.7981 - val_acc: 0.7600\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.4451 - acc: 0.8093 - val_loss: 0.6610 - val_acc: 0.6800\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.3329 - acc: 0.8529 - val_loss: 0.6623 - val_acc: 0.7717\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.2581 - acc: 0.8843 - val_loss: 0.5781 - val_acc: 0.7283\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 15s 3s/step - loss: 2.0000 - broadcast_3_loss: 1.0000 - broadcast_4_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:26<00:00, 16.13it/s]\n",
            "  0%|          | 2/600 [00:00<00:39, 15.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:36<00:00, 16.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________3.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 1.2624 - acc: 0.6257 - val_loss: 1.1855 - val_acc: 0.7667\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.7579 - acc: 0.7529 - val_loss: 0.9332 - val_acc: 0.6017\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.5348 - acc: 0.7557 - val_loss: 0.7774 - val_acc: 0.7783\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.4342 - acc: 0.8243 - val_loss: 0.6117 - val_acc: 0.7367\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.3108 - acc: 0.8714 - val_loss: 0.5807 - val_acc: 0.7583\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 21s 4s/step - loss: 2.0000 - broadcast_5_loss: 1.0000 - broadcast_6_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [02:17<00:00, 10.02it/s]\n",
            "  0%|          | 1/600 [00:00<01:01,  9.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [01:00<00:00,  9.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________4.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 17s 12ms/step - loss: 0.7028 - acc: 0.7150 - val_loss: 0.6113 - val_acc: 0.7850\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.4164 - acc: 0.8336 - val_loss: 0.5809 - val_acc: 0.7233\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.2882 - acc: 0.8757 - val_loss: 0.5715 - val_acc: 0.7700\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.1987 - acc: 0.9407 - val_loss: 0.5875 - val_acc: 0.7600\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.1497 - acc: 0.9657 - val_loss: 0.5760 - val_acc: 0.7667\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 21s 3s/step - loss: 2.0000 - broadcast_7_loss: 1.0000 - broadcast_8_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:57<00:00, 11.90it/s]\n",
            "  0%|          | 2/600 [00:00<00:47, 12.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:48<00:00, 12.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________5.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 0.8301 - acc: 0.6893 - val_loss: 0.6932 - val_acc: 0.6750\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.4826 - acc: 0.7864 - val_loss: 0.5806 - val_acc: 0.7567\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.3359 - acc: 0.8593 - val_loss: 0.5935 - val_acc: 0.7717\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.2490 - acc: 0.9036 - val_loss: 0.5926 - val_acc: 0.7100\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.2030 - acc: 0.9436 - val_loss: 0.5792 - val_acc: 0.7717\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 17s 3s/step - loss: 2.0000 - broadcast_9_loss: 1.0000 - broadcast_10_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:39<00:00, 14.06it/s]\n",
            "  0%|          | 2/600 [00:00<00:39, 15.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:39<00:00, 15.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________6.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 15s 11ms/step - loss: 0.9306 - acc: 0.6571 - val_loss: 0.7648 - val_acc: 0.7567\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 0.5262 - acc: 0.7750 - val_loss: 0.6874 - val_acc: 0.7117\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 0.3463 - acc: 0.8514 - val_loss: 0.6294 - val_acc: 0.7683\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 0.2302 - acc: 0.9143 - val_loss: 0.5977 - val_acc: 0.7583\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 0.1655 - acc: 0.9514 - val_loss: 0.5881 - val_acc: 0.7733\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 19s 3s/step - loss: 2.0000 - broadcast_11_loss: 1.0000 - broadcast_12_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:44<00:00, 13.44it/s]\n",
            "  0%|          | 2/600 [00:00<00:40, 14.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:42<00:00, 14.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________7.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 13s 10ms/step - loss: 0.8212 - acc: 0.6550 - val_loss: 0.7908 - val_acc: 0.7367\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 13s 9ms/step - loss: 0.4964 - acc: 0.7671 - val_loss: 0.6726 - val_acc: 0.7400\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 13s 9ms/step - loss: 0.3707 - acc: 0.8364 - val_loss: 0.6520 - val_acc: 0.7100\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.2624 - acc: 0.8971 - val_loss: 0.6519 - val_acc: 0.7617\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 13s 9ms/step - loss: 0.1920 - acc: 0.9407 - val_loss: 0.6018 - val_acc: 0.7333\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 17s 3s/step - loss: 2.0000 - broadcast_13_loss: 1.0000 - broadcast_14_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [01:32<00:00, 15.11it/s]\n",
            "  0%|          | 2/600 [00:00<00:37, 15.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:37<00:00, 16.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________8.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 18s 13ms/step - loss: 1.9810 - acc: 0.5457 - val_loss: 1.9985 - val_acc: 0.7450\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 17s 12ms/step - loss: 1.5549 - acc: 0.7821 - val_loss: 1.1393 - val_acc: 0.7500\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 17s 12ms/step - loss: 0.6661 - acc: 0.7779 - val_loss: 0.9365 - val_acc: 0.6417\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 17s 12ms/step - loss: 0.5650 - acc: 0.7643 - val_loss: 0.7164 - val_acc: 0.7450\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 17s 12ms/step - loss: 0.4045 - acc: 0.8371 - val_loss: 0.6700 - val_acc: 0.7517\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 23s 4s/step - loss: 2.0000 - broadcast_15_loss: 1.0000 - broadcast_16_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [02:04<00:00, 11.25it/s]\n",
            "  0%|          | 2/600 [00:00<00:49, 12.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:50<00:00, 11.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________9.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 28s 20ms/step - loss: 1.4782 - acc: 0.5793 - val_loss: 1.4921 - val_acc: 0.7383\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 19s 13ms/step - loss: 0.9437 - acc: 0.7536 - val_loss: 0.8284 - val_acc: 0.6350\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 19s 13ms/step - loss: 0.6714 - acc: 0.6893 - val_loss: 0.6577 - val_acc: 0.7533\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 19s 13ms/step - loss: 0.4746 - acc: 0.8057 - val_loss: 0.6747 - val_acc: 0.7500\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 19s 13ms/step - loss: 0.3285 - acc: 0.8621 - val_loss: 0.6422 - val_acc: 0.7117\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6/6 [==============================] - 28s 5s/step - loss: 2.0000 - broadcast_17_loss: 1.0000 - broadcast_18_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [02:16<00:00, 10.28it/s]\n",
            "  0%|          | 2/600 [00:00<00:55, 10.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:54<00:00, 10.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____________________10.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 723 samples, validate on 311 samples\n",
            "Epoch 1/5\n",
            "723/723 [==============================] - 20s 28ms/step - loss: 3.6105 - acc: 0.7704 - val_loss: 3.5670 - val_acc: 0.7621\n",
            "Epoch 2/5\n",
            "723/723 [==============================] - 19s 26ms/step - loss: 3.0766 - acc: 0.7718 - val_loss: 2.1378 - val_acc: 0.7588\n",
            "Epoch 3/5\n",
            "723/723 [==============================] - 16s 22ms/step - loss: 1.4600 - acc: 0.7649 - val_loss: 0.9517 - val_acc: 0.5691\n",
            "Epoch 4/5\n",
            "723/723 [==============================] - 16s 22ms/step - loss: 1.2343 - acc: 0.4965 - val_loss: 0.9630 - val_acc: 0.5627\n",
            "Epoch 5/5\n",
            "723/723 [==============================] - 16s 22ms/step - loss: 0.6291 - acc: 0.7372 - val_loss: 0.8459 - val_acc: 0.7492\n",
            "__making the analyzers__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:130: RuntimeWarning: This analyzer does not need to be trained. Still fit() is called.\n",
            "  \" Still fit() is called.\", RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "3/3 [==============================] - 22s 7s/step - loss: 2.0000 - broadcast_19_loss: 1.0000 - broadcast_20_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/723 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 723/723 [01:48<00:00,  7.12it/s]\n",
            "  0%|          | 1/311 [00:00<00:43,  7.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on validation set__\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 311/311 [00:43<00:00,  7.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J8Bgab5ixcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a3764f9-feca-434d-84f2-1789008d22a6"
      },
      "source": [
        "sent[0]['отличный_ADJ']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.635256052250043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SB_E_IxYtPIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec9e605b-e07b-4717-a0a6-3245cbfbbd1e"
      },
      "source": [
        "sent[0]['ужасный_ADJ']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.327932534972206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UMJSjs1_tPI4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "009e3fd0-5623-4da3-f5c2-a3bfed21ac81"
      },
      "source": [
        "sent[0]['хорошо_ADV']"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.924667179817334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_CEOsultPI8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "894d1ee3-51bc-4b7f-b956-68a6a549e8cf"
      },
      "source": [
        "sent[0]['плохо_ADV']"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.546172444941476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMFCnGN91RuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/studies/HSE/prog/nlp/sent_food.pickle', 'rb') as f:\n",
        "  sent = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saa6i97I1YwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31a320fb-ae24-46bf-be28-067aecdff381"
      },
      "source": [
        "sent[0]['плохо_ADV']"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.546172444941476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfzfbUNR75oe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58ee85be-92b4-4c5e-94ea-21f27233d00f"
      },
      "source": [
        "sent[0]['хорошо_ADV']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.924667179817334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2mKEY_0w1GLh"
      },
      "source": [
        "## Service\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZVkGrX45uwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lilrJIa_1GLr",
        "colab": {}
      },
      "source": [
        "scores = scored[\"service\"].apply(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0756bc62-bdf5-486e-fe41-a29280e6a99b",
        "id": "PKoyxb9o1GLx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "sent = get_sentiment_vocabulary(w2v_model, texts[:2000], scores[:2000], epochs=5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 13s 10ms/step - loss: 0.9575 - acc: 0.6579 - val_loss: 0.6883 - val_acc: 0.6717\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.5450 - acc: 0.7521 - val_loss: 0.6064 - val_acc: 0.7783\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.3737 - acc: 0.8436 - val_loss: 0.5578 - val_acc: 0.7333\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.2947 - acc: 0.8807 - val_loss: 0.5454 - val_acc: 0.7883\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 11s 8ms/step - loss: 0.2155 - acc: 0.9200 - val_loss: 0.5049 - val_acc: 0.7850\n",
            "__making the analyzers__\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 16s 3s/step - loss: 2.0000 - broadcast_21_loss: 1.0000 - broadcast_22_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__: 100%|██████████| 1400/1400 [01:26<00:00, 16.19it/s]\n",
            "__building the sentiment vocabulary on validation set__: 100%|██████████| 600/600 [00:33<00:00, 18.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "725d6bf3-9b39-4239-e65b-5a34c49d079e",
        "id": "f2erBPXO1GL2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sent[0]['отличный_ADJ']"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-14.341200320981443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7d3a11c-a316-4b57-a024-dd1a0f574ac5",
        "id": "QEgz8mAv1GL6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sent[0]['ужасный_ADJ']"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9018306899815798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7236f40d-4201-4436-bc2f-b898b32a3774",
        "id": "91_0-5981GL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sent[0]['хорошо_ADV']"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.343268582597375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "267410c2-3c74-4ace-a6e2-6d00550ecc00",
        "id": "6ncLBFuQ1GMA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sent[0]['плохо_ADV']"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4736492922529578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "06b491ac-4c94-4a55-e3e2-3d6ad3672f7b",
        "id": "jnGS1elr1GME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "for i in range(2000,len(texts),2000):\n",
        "  print(f\"____________________{i/2000}_________________________\")\n",
        "  if i + 2000 > len(texts):\n",
        "    sent = get_sentiment_vocabulary(w2v_model, texts[i:], scores[i:], sent, epochs=5)\n",
        "  else:\n",
        "    sent = get_sentiment_vocabulary(w2v_model, texts[i:i+2000], scores[i:i+2000], sent, epochs=5)\n",
        "  with open('/content/drive/My Drive/studies/HSE/prog/nlp/sent_service.pickle', 'wb') as f:\n",
        "    pickle.dump(sent, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____________________1.0_________________________\n",
            "__making the dataset__\n",
            "__training the network__\n",
            "Train on 1400 samples, validate on 600 samples\n",
            "Epoch 1/5\n",
            "1400/1400 [==============================] - 14s 10ms/step - loss: 1.2474 - acc: 0.6200 - val_loss: 0.6655 - val_acc: 0.7483\n",
            "Epoch 2/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.6939 - acc: 0.6593 - val_loss: 0.5367 - val_acc: 0.7700\n",
            "Epoch 3/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.4986 - acc: 0.7979 - val_loss: 0.4945 - val_acc: 0.7917\n",
            "Epoch 4/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.3314 - acc: 0.8564 - val_loss: 0.5072 - val_acc: 0.7600\n",
            "Epoch 5/5\n",
            "1400/1400 [==============================] - 12s 9ms/step - loss: 0.2231 - acc: 0.9129 - val_loss: 0.4638 - val_acc: 0.8117\n",
            "__making the analyzers__\n",
            "Epoch 1/1\n",
            "6/6 [==============================] - 18s 3s/step - loss: 2.0000 - broadcast_23_loss: 1.0000 - broadcast_24_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "__building the sentiment vocabulary on training set__:  48%|████▊     | 675/1400 [00:51<00:44, 16.34it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C9UsqSAp1GMG",
        "colab": {}
      },
      "source": [
        "sent[0]['отличный_ADJ']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySHppeSw1GMJ",
        "colab": {}
      },
      "source": [
        "sent[0]['ужасный_ADJ']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhLS2Api1GML",
        "colab": {}
      },
      "source": [
        "sent[0]['хорошо_ADV']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnZ86GEt1GMO",
        "colab": {}
      },
      "source": [
        "sent[0]['плохо_ADV']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
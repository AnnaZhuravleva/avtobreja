{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_lists_expanding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaZhuravleva/avtobreja/blob/master/word_lists_expanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLij5UzriD2",
        "colab_type": "code",
        "outputId": "3bc09f08-0e4c-4215-ffa8-e15e1f835664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "elmo_path = '/content/drive/My Drive/Colab Notebooks/nlp'\n",
        "project_path = '/content/drive/My Drive/Colab Notebooks/nlp'\n",
        "sys.path.append(project_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ws_8cWrpR2",
        "colab_type": "code",
        "outputId": "207b6b1a-9b41-4bad-bedb-a15159432e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "import urllib\n",
        "# urllib.request.urlretrieve(\"https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\", \"/content/drive/My Drive/Colab Notebooks/nlp/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\")\n",
        "!pip install git+https://github.com/lopuhin/python-adagram.git\n",
        "!pip install pymorphy2\n",
        "import adagram\n",
        "# from allennlp.commands.elmo import ElmoEmbedder\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from pymystem3 import Mystem\n",
        "import tqdm\n",
        "import nltk\n",
        "import json\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "\n",
        "!pip install wiki-ru-wordnet\n",
        "from wiki_ru_wordnet import WikiWordnet\n",
        "wikiwordnet = WikiWordnet()\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "token = RegexpTokenizer('\\w+')\n",
        "\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    words = [morph.parse(word)[0] for word in tokenize(text) if word not in stops]\n",
        "    return words\n",
        "\n",
        "def tokenize(text):\n",
        "    return token.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-llswg1_i\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-llswg1_i\n",
            "Requirement already satisfied (use --upgrade to upgrade): adagram==0.0.1 from git+https://github.com/lopuhin/python-adagram.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=464612 sha256=da72aa28fd8ba5a6d994f9be5fa74fa92df98377f034bd7d9f82b2222f7cc9e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gelwc4sw/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.393442.3710985)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: wiki-ru-wordnet in /usr/local/lib/python3.6/dist-packages (1.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUSpxbka3PQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"/content/drive/My Drive/Colab Notebooks/nlp/ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfCR2i2f3_ja",
        "colab_type": "code",
        "outputId": "9492971a-b170-4336-d8dd-a01706c62a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import gensim\n",
        "m = '/content/drive/My Drive/Colab Notebooks/nlp/ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
        "model.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0c_q9CtrquT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "POS = defaultdict()\n",
        "POS['INFN'] = 'V'\n",
        "POS['ADJF'] = 'A'\n",
        "POS['NOUN'] = 'S'\n",
        "POS['VERB'] = 'V'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AtmNBKmrt3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def lemmas(path):\n",
        "    words = []\n",
        "    marks = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for idx, line in enumerate(reader):\n",
        "            try:\n",
        "                parsed = normalize(line[0])\n",
        "                if len(parsed) > 0 and parsed[0].tag.POS in POS.keys() :\n",
        "                    tmp = str(parsed[0].normal_form) + '_' + POS[str(parsed[0].tag.POS)]\n",
        "                    words.append(tmp)\n",
        "                    marks.append(line[1])\n",
        "            except Exception as e:\n",
        "                print(e, line)\n",
        "    words = words[:4000] + words[-4000:]\n",
        "    marks = [0] * 4000 + [1] * 4000\n",
        "    return words, marks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75YiwozS6sLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vm = adagram.VectorModel.load('/content/drive/My Drive/Colab Notebooks/nlp/all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWekR0UaCEWp",
        "colab_type": "text"
      },
      "source": [
        "**wikiwordnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZF0T-EB6yR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wwn(lemma):\n",
        "  wwn = []\n",
        "  synsets = wikiwordnet.get_synsets(lemma)\n",
        "  for syns in synsets:\n",
        "    wwn += [a.lemma() for a in syns.get_words()]\n",
        "    hyponims = wikiwordnet.get_hyponyms(syns)\n",
        "    for hyp in hyponims:\n",
        "      wwn += [a.lemma() for a in hyp.get_words()]\n",
        "  return set(wwn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEyGOrP-Dpt_",
        "colab_type": "text"
      },
      "source": [
        "**word2vek**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQiA_9e5DcfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def w2v(lemma):\n",
        "  w2v_list = []\n",
        "  if lemma in model:\n",
        "      for i in model.most_similar(positive=[lemma], topn=10):\n",
        "          w2v_list.append(i[0].split('_')[0])\n",
        "      else:\n",
        "        pass\n",
        "  return set(w2v_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ar_CE4VIp_y",
        "colab_type": "text"
      },
      "source": [
        "**adagram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBexVTZWIsjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adagr(lemma):\n",
        "  result = []\n",
        "  try:\n",
        "      senses = len(vm.word_sense_probs(lemma))\n",
        "      for i in range(senses):\n",
        "          result += vm.sense_neighbors(lemma, i)\n",
        "  except:\n",
        "      pass\n",
        "  return set(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-fbcwh1Tf8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "def expand_list(path):\n",
        "    lems, marks = list(lemmas(path))\n",
        "    print(lems)\n",
        "    w2v_list, wwn_list, adagr_list, all_lists = deepcopy(lems), deepcopy(lems), deepcopy(lems), {i:j for i,j in zip(lems, marks)}\n",
        "    i = 0\n",
        "    for mark, word in zip(marks, lems):\n",
        "        print(i)\n",
        "        i += 1\n",
        "        l1, l2, l3 = w2v(word), wwn(word[:-2]), adagr(word[:-2])\n",
        "        w2v_list += l1\n",
        "        wwn_list += l2\n",
        "        adagr_list += l3\n",
        "        for a in set(list(l1)+list(l2)+list(l3)):\n",
        "            all_lists[a] = mark\n",
        "    print(set(w2v_list), set(wwn_list), set(adagr_list))\n",
        "    result = set(w2v_list) & set(wwn_list) & set(adagr_list)\n",
        "    marked_result = {a: all_lists[a] for a in list(result)}\n",
        "    return marked_result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEuayGauPp8G",
        "colab_type": "text"
      },
      "source": [
        "**составляем списки**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLb1C8K4rv-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "food = '/content/drive/My Drive/Colab Notebooks/nlp/word_lists/semantic_axis_method/food.csv'\n",
        "service = '/content/drive/My Drive/Colab Notebooks/nlp/word_lists/semantic_axis_method/service.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OlS7FSMMBjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import json\n",
        "#service_l = expand_list(service)\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/service.txt', 'w') as outfile:\n",
        "#    json.dump(service_l, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c-928jtkoxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import json\n",
        "#food_l = expand_list(food)\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/food.txt', 'w') as outfile:\n",
        "#    json.dump(food_l, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtdW6AnxNlhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/service.txt', 'r') as outfile:\n",
        "    serv = json.load(outfile)\n",
        "import csv\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/service.csv', 'w', encoding='utf-8') as outfile:\n",
        "      writer = csv.writer(outfile, delimiter='\\t')\n",
        "      for key in serv:\n",
        "          writer.writerow([key, serv[key]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I209gKLuzKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/food.txt', 'r') as outfile:\n",
        "    foodd = json.load(outfile)\n",
        "import csv\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/food.csv', 'w', encoding='utf-8') as outfile:\n",
        "      writer = csv.writer(outfile, delimiter='\\t')\n",
        "      for key in foodd:\n",
        "          writer.writerow([key, foodd[key]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSf1qRlTvIn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "9df00d82-ccf0-4aa9-cc2a-ae46f49a2bb2"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp/service.csv', sep='\\t').head(20)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>страждущий_A</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>экономность_S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>засаить_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>косноязычный_A</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>крискат_S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вытянутать_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>условие_S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>заболевание_S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>подтверждение_S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>детородный_A</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>знааааать_V</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>рифмевать_V</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>розмарин_S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>застлать_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ханзать_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>бог_S</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>наймть_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>аджеп_S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>потрескаться_V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>хрустение_S</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>окраинный_A</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       страждущий_A  0\n",
              "0     экономность_S  0\n",
              "1         засаить_V  1\n",
              "2    косноязычный_A  1\n",
              "3         крискат_S  1\n",
              "4      вытянутать_V  1\n",
              "5         условие_S  0\n",
              "6     заболевание_S  0\n",
              "7   подтверждение_S  0\n",
              "8      детородный_A  0\n",
              "9       знааааать_V  0\n",
              "10      рифмевать_V  0\n",
              "11       розмарин_S  1\n",
              "12       застлать_V  1\n",
              "13        ханзать_V  1\n",
              "14            бог_S  0\n",
              "15         наймть_V  1\n",
              "16          аджеп_S  1\n",
              "17   потрескаться_V  1\n",
              "18      хрустение_S  1\n",
              "19      окраинный_A  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gpDhEovPRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_lists_expanding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaZhuravleva/avtobreja/blob/master/word_lists_expanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLij5UzriD2",
        "colab_type": "code",
        "outputId": "a579fc01-2b69-4fea-dd1f-2cfe5d3bd82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "elmo_path = '/content/drive/My Drive/Colab Notebooks/nlp'\n",
        "project_path = '/content/drive/My Drive/Colab Notebooks/nlp'\n",
        "sys.path.append(project_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ws_8cWrpR2",
        "colab_type": "code",
        "outputId": "87431f6e-5d84-49f5-899b-801f8bceee9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import urllib\n",
        "# urllib.request.urlretrieve(\"https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\", \"/content/drive/My Drive/Colab Notebooks/nlp/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\")\n",
        "!pip install git+https://github.com/lopuhin/python-adagram.git\n",
        "!pip install pymorphy2\n",
        "import adagram\n",
        "# from allennlp.commands.elmo import ElmoEmbedder\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from pymystem3 import Mystem\n",
        "import tqdm\n",
        "import nltk\n",
        "import json\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "\n",
        "!pip install wiki-ru-wordnet\n",
        "from wiki_ru_wordnet import WikiWordnet\n",
        "wikiwordnet = WikiWordnet()\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "token = RegexpTokenizer('\\w+')\n",
        "\n",
        "import gensim\n",
        "m = '/content/drive/My Drive/Colab Notebooks/nlp/ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
        "model.init_sims(replace=True)\n",
        "vm = adagram.VectorModel.load('/content/drive/My Drive/Colab Notebooks/nlp/all.a010.p10.d300.w5.m100.nonorm.slim.joblib')\n",
        "\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    words = [morph.parse(word)[0] for word in tokenize(text) if word not in stops]\n",
        "    return words\n",
        "\n",
        "def tokenize(text):\n",
        "    return token.tokenize(text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-vgtsru5s\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-vgtsru5s\n",
            "Requirement already satisfied (use --upgrade to upgrade): adagram==0.0.1 from git+https://github.com/lopuhin/python-adagram.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=464615 sha256=36503864b75d99aa2836a453125fcbc2043483e5084576c687cabc8c53d96411\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wqr8jrbu/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.393442.3710985)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: wiki-ru-wordnet in /usr/local/lib/python3.6/dist-packages (1.0.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUSpxbka3PQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"/content/drive/My Drive/Colab Notebooks/nlp/ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0c_q9CtrquT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "POS = defaultdict()\n",
        "POS['INFN'] = 'V'\n",
        "POS['ADJF'] = 'A'\n",
        "POS['NOUN'] = 'S'\n",
        "POS['VERB'] = 'V'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AtmNBKmrt3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def lemmas(path):\n",
        "    words = []\n",
        "    marks = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for idx, line in enumerate(reader):\n",
        "            try:\n",
        "                parsed = normalize(line[0])\n",
        "                if len(parsed) > 0 and parsed[0].tag.POS in POS.keys() :\n",
        "                    tmp = str(parsed[0].normal_form) + '_' + POS[str(parsed[0].tag.POS)]\n",
        "                    words.append(tmp)\n",
        "                    marks.append(line[1])\n",
        "            except Exception as e:\n",
        "                print(e, line)\n",
        "    words = words[:4000] + words[-4000:]\n",
        "    marks = [0] * 4000 + [1] * 4000\n",
        "    return words, marks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWekR0UaCEWp",
        "colab_type": "text"
      },
      "source": [
        "**wikiwordnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZF0T-EB6yR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wwn(lemma):\n",
        "  wwn = []\n",
        "  synsets = wikiwordnet.get_synsets(lemma)\n",
        "  for syns in synsets:\n",
        "    wwn += [a.lemma() for a in syns.get_words()]\n",
        "    hyponims = wikiwordnet.get_hyponyms(syns)\n",
        "    for hyp in hyponims:\n",
        "      wwn += [a.lemma() for a in hyp.get_words()]\n",
        "  return set(wwn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEyGOrP-Dpt_",
        "colab_type": "text"
      },
      "source": [
        "**word2vek**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQiA_9e5DcfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def w2v(lemma):\n",
        "  w2v_list = []\n",
        "  if lemma in model:\n",
        "      for i in model.most_similar(positive=[lemma], topn=10):\n",
        "          w2v_list.append(i[0].split('_')[0])\n",
        "      else:\n",
        "        pass\n",
        "  return set(w2v_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ar_CE4VIp_y",
        "colab_type": "text"
      },
      "source": [
        "**adagram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBexVTZWIsjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adagr(lemma):\n",
        "  result = []\n",
        "  try:\n",
        "      senses = len(vm.word_sense_probs(lemma))\n",
        "      for i in range(senses):\n",
        "          result += vm.sense_neighbors(lemma, i)\n",
        "  except:\n",
        "      pass\n",
        "  return set(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-fbcwh1Tf8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "def expand_list(path):\n",
        "    lems, marks = list(lemmas(path))\n",
        "    print(lems)\n",
        "    w2v_list, wwn_list, adagr_list, all_lists = deepcopy(lems), deepcopy(lems), deepcopy(lems), {i:j for i,j in zip(lems, marks)}\n",
        "    i = 0\n",
        "    for mark, word in zip(marks, lems):\n",
        "        print(i)\n",
        "        i += 1\n",
        "        l1, l2, l3 = w2v(word), wwn(word[:-2]), adagr(word[:-2])\n",
        "        w2v_list += l1\n",
        "        wwn_list += l2\n",
        "        adagr_list += l3\n",
        "        for a in set(list(l1)+list(l2)+list(l3)):\n",
        "            all_lists[a] = mark\n",
        "    print(set(w2v_list), set(wwn_list), set(adagr_list))\n",
        "    result = set(w2v_list) & set(wwn_list) & set(adagr_list)\n",
        "    marked_result = {a: all_lists[a] for a in list(result)}\n",
        "    return marked_result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEuayGauPp8G",
        "colab_type": "text"
      },
      "source": [
        "**составляем списки**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLb1C8K4rv-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "food = '/content/drive/My Drive/Colab Notebooks/nlp/word_lists/semantic_axis_method/food.csv'\n",
        "service = '/content/drive/My Drive/Colab Notebooks/nlp/word_lists/semantic_axis_method/service.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OlS7FSMMBjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import json\n",
        "#service_l = expand_list(service)\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/service.txt', 'w') as outfile:\n",
        "#    json.dump(service_l, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c-928jtkoxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import json\n",
        "#food_l = expand_list(food)\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/food.txt', 'w') as outfile:\n",
        "#    json.dump(food_l, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtdW6AnxNlhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import json\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/service.txt', 'r') as outfile:\n",
        "#    serv = json.load(outfile)\n",
        "#import csv\n",
        "#with open('/content/drive/My Drive/Colab Notebooks/nlp/service.csv', 'w', encoding='utf-8') as outfile:\n",
        "#      writer = csv.writer(outfile, delimiter='\\t')\n",
        "#      for key in serv:\n",
        "#          writer.writerow([key, serv[key]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSf1qRlTvIn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas as pd\n",
        "#pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp/service.csv', sep='\\t').head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gpDhEovPRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmas_keys(path):\n",
        "    words = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for idx, line in enumerate(f.readlines()):\n",
        "            try:\n",
        "                parsed = normalize(line.strip('\\n'))\n",
        "                if len(parsed) > 0 and parsed[0].tag.POS in POS.keys() :\n",
        "                    tmp = str(parsed[0].normal_form) + '_' + POS[str(parsed[0].tag.POS)]\n",
        "                    words.append(tmp)\n",
        "            except Exception as e:\n",
        "                print(e, line)\n",
        "    return words\n",
        "\n",
        "from copy import deepcopy\n",
        "def expand_list_keys(path):\n",
        "    lems = list(lemmas_keys(path))\n",
        "    raw_lems = [i[:-2] for i in lems]\n",
        "    w2v_list, wwn_list, adagr_list = deepcopy(raw_lems), deepcopy(raw_lems), deepcopy(raw_lems)\n",
        "    i = 0\n",
        "    for word in lems:\n",
        "        l1, l2, l3 = w2v(word), wwn(word[:-2]), adagr(word[:-2])\n",
        "        w2v_list += l1\n",
        "        wwn_list += l2\n",
        "        adagr_list += [i[0] for i in l3]\n",
        "    result = w2v_list +  adagr_list\n",
        "    result = set(result) & set(wwn_list)\n",
        "    return list(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcLDvvrY-1F8",
        "colab_type": "code",
        "outputId": "8e13cfad-34fd-417f-9be9-347f7cc30d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#food_keys = expand_list_keys('/content/drive/My Drive/Colab Notebooks/nlp/word_lists/food_key.txt')\n",
        "service_keys = expand_list_keys('/content/drive/My Drive/Colab Notebooks/nlp/word_lists/service_key.txt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/adagram/model.py:124: RuntimeWarning: invalid value encountered in true_divide\n",
            "  sim_matrix = np.dot(self.In, s_v) / self.InNorms\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXojkteFAZMr",
        "colab_type": "code",
        "outputId": "10ed876f-2e2a-42fd-a807-1a0c93f4a3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(food_keys)\n",
        "print(service_keys)\n",
        "print(len(food_keys), len(service_keys))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['бутерброд', 'овощ', 'рагу', 'кушанье', 'батон', 'жаркое', 'напиток', 'суп', 'алкоголь', 'десерт', 'кляр', 'повар', 'сыр', 'кетчуп', 'тарелка', 'пицца', 'готовить', 'яство', 'паста', 'состав', 'спирт', 'говядина', 'варить', 'щука', 'кофе', 'гребешок', 'борщ', 'просфора', 'ингредиент', 'зажаривать', 'гуляш', 'тефтели', 'плюшка', 'ветчина', 'пшеница', 'запеканка', 'булочка', 'хлеб', 'баранина', 'морс', 'колбаса', 'выпечка', 'лимонад', 'квас', 'соус', 'молоко', 'печень', 'сок', 'свинина', 'ликёр', 'вино', 'цезарь', 'лосось', 'тунец', 'компот', 'подливка', 'бульон', 'нектар', 'блюдо', 'пунш', 'снедь', 'сосиска', 'обед', 'шампанское', 'краб', 'питие', 'пирог', 'стряпать', 'окунь', 'ужин', 'коктейль', 'песто', 'член', 'кальмар', 'поджаривать', 'жратва', 'пища', 'рыба', 'жарить', 'цена', 'мясо', 'японский', 'пирожок', 'плов', 'яичница', 'порция', 'коньяк', 'сухарь', 'форель', 'салат', 'макароны', 'котлета', 'йогурт', 'вермут', 'винегрет', 'кухня', 'завтрак', 'кока-кола', 'лисичка', 'гренок', 'еда', 'конина', 'каша', 'отварный', 'выпивка', 'кролик', 'ситро', 'креветка', 'кусок', 'чипсы', 'европейский', 'пиво', 'виски', 'булка', 'кисель', 'млеко']\n",
            "['официант', 'обслуживание', 'услуга', 'подсказывать', 'администрация', 'приносить', 'официантка', 'логистика', 'посоветовать', 'управление', 'руководство', 'персонал', 'пожелание', 'командование', 'советовать', 'транспорт', 'обслуживать', 'админ', 'рекомендовать', 'администратор', 'сервис', 'самоуправление']\n",
            "116 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "835WaoRwBtWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/word_lists/service_key_extended.txt','w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(service_keys))\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nlp/word_lists/food_key_extended.txt','w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(food_keys))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6o3fsDFJGIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}